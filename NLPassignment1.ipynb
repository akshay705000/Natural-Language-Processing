{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment No 1**\n",
        "\n",
        "**Group members :**\n",
        "\n",
        "1) Akshy Kumar (CS23MTECH11022)\n",
        "\n",
        "2) Arnab Ghosh (CS23MTECH11025)\n",
        "\n",
        "3) Sanket Rathod (CS23MTECH11033)"
      ],
      "metadata": {
        "id": "F1DKLQ6UL3iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question No 1\n",
        "Implement BLEU Score metric. Pre-process the text by lower-casing the text and removing\n",
        "punctuation."
      ],
      "metadata": {
        "id": "4WBepwVpMaWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xucDIIrmMvTA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for preprocessing the sentences.\n",
        "def preprocess(sentence):\n",
        "    # Converting text to lower case.\n",
        "    sentence = sentence.lower()\n",
        "    # Removing Punctuation from the text.\n",
        "    sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    # Splitting the text and returning the list of splitted sentence.\n",
        "    return sentence.split()"
      ],
      "metadata": {
        "id": "VVX4ktFrMx9Z"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting ngrams by passing Tokens and value of n.\n",
        "def count_ngrams(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = tuple(tokens[i:i + n])\n",
        "        ngrams.append(ngram)\n",
        "    # Returning the frequency of each token in a sentence using Counter function.\n",
        "    return Counter(ngrams)"
      ],
      "metadata": {
        "id": "p85-tvMsM1qi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here calculating Modified ngram precision.\n",
        "def clipped_precision(reference_counts, candidate_counts):\n",
        "    clipped_counts = {}\n",
        "    for ngram, count in candidate_counts.items():\n",
        "        if ngram in reference_counts:\n",
        "            clipped_counts[ngram] = min(count, reference_counts[ngram])\n",
        "    # if not any mathch found then return precision as 0.\n",
        "    if not clipped_counts or not candidate_counts:\n",
        "        return 0\n",
        "    # returning the modified ngram precision\n",
        "    return sum(clipped_counts.values()) / sum(candidate_counts.values())"
      ],
      "metadata": {
        "id": "lhE2cJJNM5DW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating bleu score.\n",
        "def bleu_score(reference, candidate, n=4):\n",
        "    # Preprocessing Refrence text\n",
        "    reference_tokens = preprocess(reference)\n",
        "    # Preprocessing Candidate text\n",
        "    candidate_tokens = preprocess(candidate)\n",
        "    # Calculating the length of refrence tokens and candidate tokens.\n",
        "    reference_length = len(reference_tokens)\n",
        "    candidate_length = len(candidate_tokens)\n",
        "\n",
        "    reference_counts = {ngram: count for n in range(1, n + 1) for ngram, count in count_ngrams(reference_tokens, n).items()}\n",
        "    candidate_counts = {ngram: count for n in range(1, n + 1) for ngram, count in count_ngrams(candidate_tokens, n).items()}\n",
        "    # Precision Calculation\n",
        "    precision = clipped_precision(reference_counts, candidate_counts)\n",
        "    # Keeping BP value as 1 as mensioned in question.\n",
        "    bp = 1\n",
        "\n",
        "    BLEU = bp * np.exp(np.log(precision + 1e-10) / n)\n",
        "    return BLEU"
      ],
      "metadata": {
        "id": "7M62tuHaM7PK"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing text by lower-casing the text and removing punctuation\n",
        "text = \"The boys were playing happily on the ground.\"\n",
        "preprocessed_text = preprocess(text)\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYlBRSgMPmoY",
        "outputId": "092db786-9f93-42c8-ee41-f17b1d219d5b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'boys', 'were', 'playing', 'happily', 'on', 'the', 'ground']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question no 2\n",
        " Use this implementation to find BLEU Score when\n",
        "\n",
        "x = ”The boys were playing happily on the\n",
        "ground.” and\n",
        "\n",
        "y = ”The boys were playing football on the field.”."
      ],
      "metadata": {
        "id": "-UeOKhhvO251"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing BLEU score for these texts.\n",
        "reference = \"The boys were playing happily on the ground.\"\n",
        "candidate = \"The boys were playing football on the field.\"\n",
        "# passing refrence and candidate\n",
        "bleu_score_value = bleu_score(reference, candidate)\n",
        "print(\"BLEU Score:\", bleu_score_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3it06fRM-AG",
        "outputId": "f0e67dd5-beb5-4745-e8b8-f93e2516edde"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.8408964152957594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question no 3\n",
        "\n",
        " Can you explain why we are taking minimum in numerator in equation 1?"
      ],
      "metadata": {
        "id": "PS8V_YGtQTw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expression min(count-x(n-gram),count-y(n-gram)) ensures that the modified n-gram precision considers only the minimum count of occurrences between the machine-translated text and the reference text.\n",
        "\n",
        "We are taking minimum in numerator of equation 1 given in the question because penalize the machine translated text in the two situations as given below:\n",
        "\n",
        "\n",
        "* If the machine-translated text generates more occurrences of an\n",
        "n-gram then what it is actually present in the reference text then it should only be evaluated based on the number of occurrences in the reference text to avoid artificial inflation of precision.\n",
        "\n",
        "* If the machine-translated text generates fewer occurrences of an n-gram compared to the reference text, it should be penalized based on the reference count to encourage the generation of more accurate translations.\n",
        "\n",
        "\n",
        "By taking the minimum count, the modified n-gram precision effectively evaluates how the machine-translated text matches the reference text and penalize it for generating too many occurrences of the same n-gram or for failing to generate enough occurrences as per the reference."
      ],
      "metadata": {
        "id": "mmPfFoBPROpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question no 4\n",
        "\n",
        " Use your implementation to find BLEU Score between any 5 sentence pairs and explain what\n",
        "are potential disadvantages of using the BLEU Score."
      ],
      "metadata": {
        "id": "AvEY5_rzQ-q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentence pairs\n",
        "sentence_pairs = [\n",
        "    (\"The cat is on the mat.\", \"The mat is under the cat.\"),\n",
        "    (\"He's a talented musician.\", \"He's a skilled musician.\"),\n",
        "    (\"The movie was fantastic.\", \"The film was superb.\"),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", \"The fox jumps.\"),\n",
        "    (\"She sells seashells by the seashore.\", \"She sells seashells at the seashore.\"),\n",
        "]\n",
        "# Printing BLEU score for 5 sentence pairs.\n",
        "for reference, candidate in sentence_pairs:\n",
        "    bleu_score_value = bleu_score(reference, candidate)\n",
        "    print(f\"Reference: {reference}\")\n",
        "    print(f\"Candidate: {candidate}\")\n",
        "    print(f\"BLEU Score: {bleu_score_value}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkBKMVuNGVsp",
        "outputId": "888d6f89-6034-4b29-c982-ac23d3de1032"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference: The cat is on the mat.\n",
            "Candidate: The mat is under the cat.\n",
            "BLEU Score: 0.7896895368070302\n",
            "\n",
            "Reference: He's a talented musician.\n",
            "Candidate: He's a skilled musician.\n",
            "BLEU Score: 0.7952707288167551\n",
            "\n",
            "Reference: The movie was fantastic.\n",
            "Candidate: The film was superb.\n",
            "BLEU Score: 0.6687403050600146\n",
            "\n",
            "Reference: The quick brown fox jumps over the lazy dog.\n",
            "Candidate: The fox jumps.\n",
            "BLEU Score: 0.9036020036437299\n",
            "\n",
            "Reference: She sells seashells by the seashore.\n",
            "Candidate: She sells seashells at the seashore.\n",
            "BLEU Score: 0.8408964152957594\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limited to N-gram Matching: Consider the following translations:\n",
        "\n",
        "**Reference:** \"The cat is on the mat.\"\n",
        "\n",
        "**Candidate:** \"The mat is under the cat.\"\n",
        "\n",
        "While the candidate translation conveys the same meaning as the reference, BLEU would penalize it because the n-grams do not match exactly.\n",
        "Insensitive to Synonyms and Paraphrases:\n",
        "\n",
        "**Reference:** \"He's a talented musician.\"\n",
        "\n",
        "**Candidate:** \"He's a skilled musician.\"\n",
        "\n",
        "Both sentences convey the same meaning, but BLEU would penalize the candidate translation because it uses a synonym (\"talented\" vs. \"skilled\").\n",
        "Insensitive to Structural Differences:\n",
        "\n",
        "**Reference:** \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "**Candidate:** \"The fox jumps.\"\n",
        "\n",
        "Even though the candidate translation is correct, BLEU would likely penalize it for being too short compared to the reference.\n",
        "Reference Dependency:\n",
        "\n",
        "**Reference 1:** \"The car is red.\"\n",
        "\n",
        "**Reference 2:** \"The car is colored red.\"\n",
        "\n",
        "Depending on which reference translation is chosen, the BLEU score may vary, leading to inconsistency in evaluation.\n",
        "Lack of Context Awareness:\n",
        "\n",
        "\n",
        "Difficulty in Interpretation:\n",
        "\n",
        "BLEU Score: 0.75\n",
        "It's challenging to determine what specific aspects of the translation contributed to this score without additional information or analysis."
      ],
      "metadata": {
        "id": "ez08CuRGY1YF"
      }
    }
  ]
}