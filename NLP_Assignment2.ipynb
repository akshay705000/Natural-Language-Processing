{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment No 2**\n",
        "\n",
        "**Group members :**\n",
        "\n",
        "1) Akshy Kumar (CS23MTECH11022)\n",
        "\n",
        "2) Arnab Ghosh (CS23MTECH11025)\n",
        "\n",
        "3) Sanket Rathod (CS23MTECH11033)"
      ],
      "metadata": {
        "id": "V73UDI1mddcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question No 1\n",
        "\n",
        "Preprocess and tokenize the dataset using NLTK."
      ],
      "metadata": {
        "id": "UcdUiE8dd0FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.lm import MLE, KneserNeyInterpolated\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVsK-xI9zjbE",
        "outputId": "25ddc0fe-4ce2-4c57-bf59-5b9989ec2961"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "AXN7DtcpxI-f"
      },
      "outputs": [],
      "source": [
        "# Getting dataset\n",
        "dataset = 'Harry_Potter_all_books_preprocessed.txt'\n",
        "# Opening the given dataset in read mode ('r') with UTF-8 encoding.\n",
        "with open(dataset, 'r', encoding='utf-8') as file:\n",
        "    # reading content f file and storing it in a text variable.\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing first 500 words of dataset.\n",
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HEv9KJgyjcC",
        "outputId": "c71972ae-483c-4db1-bb2d-470843b054a1"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE BOY WHO LIVED Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much .They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense .Mr Dursley was the director of a firm called Grunnings which made drills .He was a big beefy man with hardly any neck although he did have a very large mustache .Mrs Dursley was thin and blonde and had nearly twice the usual amo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation which are in front of words.\n",
        "text = text.replace('!', '')\n",
        "text = text.replace('.', '')\n",
        "text = text.replace('?', '')\n",
        "text = text.replace('\"', '')"
      ],
      "metadata": {
        "id": "SwDnxgsY1AB1"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the given tezt\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# Extracting first 10000 words from tokens.\n",
        "tokens = tokens[:10000]\n",
        "print(\"Tokens:\", tokens[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ3YD3-Qy2Ac",
        "outputId": "d5aafffb-d2f0-42d5-d197-7e3d0e30ccb0"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['THE', 'BOY', 'WHO', 'LIVED', 'Mr', 'and', 'Mrs', 'Dursley', 'of', 'number', 'four', 'Privet', 'Drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'They', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense', 'Mr', 'Dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'Grunnings', 'which', 'made', 'drills', 'He', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', 'Mrs', 'Dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in', 'very', 'useful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store the lowercased tokens\n",
        "lowercased_tokens = []\n",
        "\n",
        "# Loop through each token in the list\n",
        "for token in tokens:\n",
        "    # Converting each token to lowercase.\n",
        "    lowercased_tokens.append(token.lower())\n",
        "# Printing tokens.\n",
        "print(\"Tokens:\", lowercased_tokens[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVYx2F13z4eu",
        "outputId": "5cff5f11-6543-432b-f5e7-3e0abe23175c"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['the', 'boy', 'who', 'lived', 'mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'they', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense', 'mr', 'dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', 'which', 'made', 'drills', 'he', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', 'mrs', 'dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in', 'very', 'useful']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stopwords = []\n",
        "\n",
        "# get list of stopwords in English\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "# Loop through each token in the list\n",
        "for token in lowercased_tokens:\n",
        "    if(token not in stop_words) :\n",
        "        removed_stopwords.append(token.lower())\n",
        "# Print removed stopword tokens\n",
        "print(\"Tokens:\", removed_stopwords[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lZT7wMi3JOw",
        "outputId": "4c583213-6e88-40be-e12d-b0c02b4d86ff"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['boy', 'lived', 'mr', 'mrs', 'dursley', 'number', 'four', 'privet', 'drive', 'proud', 'say', 'perfectly', 'normal', 'thank', 'much', 'last', 'people', 'youd', 'expect', 'involved', 'anything', 'strange', 'mysterious', 'didnt', 'hold', 'nonsense', 'mr', 'dursley', 'director', 'firm', 'called', 'grunnings', 'made', 'drills', 'big', 'beefy', 'man', 'hardly', 'neck', 'although', 'large', 'mustache', 'mrs', 'dursley', 'thin', 'blonde', 'nearly', 'twice', 'usual', 'amount', 'neck', 'came', 'useful', 'spent', 'much', 'time', 'craning', 'garden', 'fences', 'spying', 'neighbors', 'dursley', 'small', 'son', 'called', 'dudley', 'opinion', 'finer', 'boy', 'anywhere', 'dursleys', 'everything', 'wanted', 'also', 'secret', 'greatest', 'fear', 'somebody', 'would', 'discover', 'didnt', 'think', 'could', 'bear', 'anyone', 'found', 'potters', 'mrs', 'potter', 'mrs', 'dursleys', 'sister', 'hadnt', 'met', 'several', 'years', 'fact', 'mrs', 'dursley', 'pretended']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question No 2\n",
        "\n",
        " Fit two bigram language models on the text: MLE and KneserNey discounting."
      ],
      "metadata": {
        "id": "0dnIRQyXeOXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate padded bigrams and vocabulary\n",
        "train_data, padded_vocab = padded_everygram_pipeline(2, [removed_stopwords])\n",
        "\n",
        "# Fit MLE bigram language models on the text keeping it as bigram\n",
        "mle = MLE(2)\n",
        "# Training mle\n",
        "mle.fit(train_data, padded_vocab)"
      ],
      "metadata": {
        "id": "Z_hAwOKEGm9x"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, padded_vocab = padded_everygram_pipeline(2, [removed_stopwords])\n",
        "# Training Kneser-Ney model\n",
        "kneser_ney_model = KneserNeyInterpolated(2)\n",
        "kneser_ney_model.fit(train_data, padded_vocab)"
      ],
      "metadata": {
        "id": "MDvKTkpqKREG"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question No 3\n",
        "\n",
        " Use the beginning words 1. ”Harry Potter” and 2. ”Dumbledore” to generate text using both\n",
        "the language models. Keep the maximum text length as 20."
      ],
      "metadata": {
        "id": "TOMfokq_ehkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Generate text.\n",
        "def generate_text(model, text_seed, num_words, max_length=20):\n",
        "    content = text_seed\n",
        "    for _ in range(max_length):\n",
        "        # Generate possible next words\n",
        "        next_word = model.generate(text_seed=content[-2:], num_words=num_words)\n",
        "        # Appending generated word to content.\n",
        "        content.append(next_word)\n",
        "        if len(content) >= max_length:\n",
        "            break\n",
        "    return ' '.join(content)"
      ],
      "metadata": {
        "id": "PkN6yFS6MZLq"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating text for Begining word Harry potter using mle model.\n",
        "generated_text = generate_text(mle, [\"harry\", \"potter\"], 1)\n",
        "print(\"Generated Text for harry potter : \", generated_text)\n",
        "# Generating text for Begining word dumbledore using mle model.\n",
        "generated_text = generate_text(mle, [\"dumbledore\"], 1)\n",
        "print(\"Generated Text for dumbledore : \", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAUzjELNMeRK",
        "outputId": "ad7ac1dd-fc80-4add-eda4-a6c5a17de238"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text for harry potter :  harry potter wasnt sure snake wouldnt able think might going believe ickle dudleykins looked quickly another word weirdest thing saw\n",
            "Generated Text for dumbledore :  dumbledore im going anything im warning boy lived two ribs might hiding harry dudley slept knowing would day future books\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating text for Begining word Harry potter using Kneserney model.\n",
        "generated_text = generate_text(kneser_ney_model, [\"harry\", \"potter\"], 1)\n",
        "print(\"Generated Text for harry potter : \", generated_text)\n",
        "# Generating text for Begining word dumbledore using Kneserney model.\n",
        "generated_text = generate_text(kneser_ney_model, [\"dumbledore\"], 1)\n",
        "print(\"Generated Text for dumbledore : \", generated_text)"
      ],
      "metadata": {
        "id": "IAZBMKtDobkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12a58ef-fd78-4237-b304-8fe0938cff0c"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text for harry potter :  harry potter boy lived vanashig glass staring glistening brown orange puff balls light came waddling toward kitchen checking letter shouted\n",
            "Generated Text for dumbledore :  dumbledore bowed head smelting stick carried everywhere reported nations owls swooping past clutching large spotted morning hed said impatiently youd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question No 4\n",
        "\n",
        "To Implement Beam Search, you would have to find the top k most probable words given some\n",
        "context. Implement a function for this."
      ],
      "metadata": {
        "id": "2psCHIgYew5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function definition to generate top k words based given context and model.\n",
        "def probable_top_k(model, context, k):\n",
        "    # Ensuring the context is in form of list\n",
        "    context_tokens = context if isinstance(context, list) else [context]\n",
        "\n",
        "    # Calculating the probability for each word in the vocabulary based on the context provided.\n",
        "    word_scores = [(word, model.score(word, context_tokens)) for word in model.vocab if word not in ('<s>', '</s>')]\n",
        "\n",
        "    # Sorting the words by the value of probability calculated above in descending order.\n",
        "    sorted_words = sorted(word_scores, key=lambda ws: ws[1], reverse=True)\n",
        "\n",
        "    # Extracting the top k words and their probabilities.\n",
        "    top_k_words_with_probs = sorted_words[:k]\n",
        "\n",
        "    return top_k_words_with_probs\n",
        "\n",
        "# Defining context and other parameters as given in the question.\n",
        "context = [[\"harry\", \"potter\"], [\"dumbledore\"],[\"boy\"]]\n",
        "k = 5\n",
        "\n",
        "# Calling the function probable_top_k to find top k words.\n",
        "top_k_words_with_probs = probable_top_k(mle, context[0], k)\n",
        "\n",
        "print(\"Top\", k, \"most probable words:\")\n",
        "for word, prob in top_k_words_with_probs:\n",
        "    print(f\"{word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlsOKO2jqa-2",
        "outputId": "6dceaf17-f155-442a-b8ed-671607acf04f"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most probable words:\n",
            "boy\n",
            "lived\n",
            "mr\n",
            "mrs\n",
            "dursley\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question No 5\n",
        "\n",
        "Implement Beam search. Use the MLE Language model trained previously"
      ],
      "metadata": {
        "id": "BrvZ90G3e_xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the beam search function to expand only top k most probable children upto\n",
        "# depth equivalent to max_length for given context word.\n",
        "def beam_search(model, initial_context, k, max_length=10):\n",
        "    # Initializing the initial context and probability.\n",
        "    paths = [(initial_context, 0.0)]\n",
        "\n",
        "    # Looping to expand up to maximum depth.\n",
        "    for _ in range(max_length - len(initial_context)):\n",
        "        new_paths = []\n",
        "\n",
        "        # Looping to expand all currents paths.\n",
        "        for path, cum_log_prob in paths:\n",
        "            context = path[-(model.order-1):] if len(path) >= model.order-1 else path\n",
        "\n",
        "            # Generating next words and their probabilities for the current context\n",
        "            next_words_with_probs = probable_top_k(model, context, k)\n",
        "\n",
        "            # Expanding path with each possible next word\n",
        "            for next_word, log_prob in next_words_with_probs:\n",
        "                new_path = path + [next_word]\n",
        "                new_cum_log_prob = cum_log_prob + log_prob\n",
        "                new_paths.append((new_path, new_cum_log_prob))\n",
        "        # updating paths after expanding all current paths with all possible next words\n",
        "        paths = new_paths\n",
        "\n",
        "    # sorting all paths/sentences in decreasing order of probability.\n",
        "    paths.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extracting top 5 sentences based on their probabilities.\n",
        "    top_5_paths = paths[:5]\n",
        "\n",
        "    return [' '.join(path) for path, _ in top_5_paths]"
      ],
      "metadata": {
        "id": "cQHIBRrfnTLf"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question No 6\n",
        "\n",
        " Repeat part 3 using Beam Search with k=2 and depth=10. Find the 5 generated texts with\n",
        "highest probability for each of the 3 sentences."
      ],
      "metadata": {
        "id": "FAgZpKcyfMnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initalizing initial contexts and other parameters.\n",
        "initial_context_1 = ['harry potter']\n",
        "initial_context_2 = ['dumbledore']\n",
        "k = 2\n",
        "depth = 10\n",
        "\n",
        "# Calling the function for the first initial context on mle model.\n",
        "print(f\"Initial context word - {initial_context_1}:\")\n",
        "top_sentences = beam_search(mle, initial_context_1, k, depth)\n",
        "\n",
        "# Printing the top 5 sentences.\n",
        "for i, sentence in enumerate(top_sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n",
        "\n",
        "# Calling the function for the second initial context on mle model.\n",
        "print(f\"Initial context word - {initial_context_2}:\")\n",
        "top_sentences = beam_search(mle, initial_context_2, k, depth)\n",
        "\n",
        "# Printing the top 5 sentences.\n",
        "for i, sentence in enumerate(top_sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiV1AZlufWGD",
        "outputId": "1fa2a7a9-6bd7-49e8-c55b-6ea618d3f185"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial context word - ['harry potter']:\n",
            "Sentence 1: harry potter lived mr dursley wondered whether dared ask questions first\n",
            "Sentence 2: harry potter lived mr dursley wondered whether dared ask questions snapped\n",
            "Sentence 3: harry potter boy lived mr dursley wondered whether dared ask questions\n",
            "Sentence 4: harry potter boy dudley tantrum throwing cereal walls behind mrs dursley\n",
            "Sentence 5: harry potter boy dudley tantrum throwing cereal walls little tyke chortled\n",
            "Initial context word - ['dumbledore']:\n",
            "Sentence 1: dumbledore sir nothing man hardly changed sleeping pattern newscaster allowed\n",
            "Sentence 2: dumbledore though shed several important telephone calls shouted uncle vernon\n",
            "Sentence 3: dumbledore sir house four privet drive proud say perfectly normal\n",
            "Sentence 4: dumbledore though wasnt bad news mr dursley wondered whether dared\n",
            "Sentence 5: dumbledore though shed ever seen privet drive proud say something\n"
          ]
        }
      ]
    }
  ]
}